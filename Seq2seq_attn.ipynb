{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Seq2seq_attn.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Qf15MqOxW5Z1","colab_type":"code","outputId":"eb9666f9-02cd-41f4-8d2f-1b8a6bb9f16b","executionInfo":{"status":"ok","timestamp":1577710056451,"user_tz":-60,"elapsed":31073,"user":{"displayName":"Asbjørn Wulff Helge","photoUrl":"","userId":"00062630824514689988"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["### Second part of the project: Making a seq2seq model and test the model on CNN data.\n","import os, sys\n","import csv\n","csv.field_size_limit(sys.maxsize)\n","import string\n","import torchtext\n","import spacy\n","import pickle\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import time\n","import torch\n","import numpy as np\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn.init as init\n","import torch.utils.data as data_util\n","\n","not_twice = False # Parameter for only loading data once\n","\n","## Use this to mount google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","os.chdir(\"/content/gdrive/My Drive/Deep Learning/VJ_og_AH/Data\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SAhjy6nTd8Ih","colab_type":"code","colab":{}},"source":["## Functions for making Cuda available\n","def get_variable(x):\n","    \"\"\" Converts tensors to cuda, if available. \"\"\"\n","    if torch.cuda.is_available():\n","        return x.cuda()\n","    return x\n","\n","def get_numpy(x):\n","    \"\"\" Get numpy array for both cuda and not. \"\"\"\n","    if torch.cuda.is_available():\n","        return x.cpu().data.numpy()\n","    return x.data.numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5w33o4jW5Z7","colab_type":"code","outputId":"c1736edc-24d2-4d13-fbc1-8e66cf6d6031","executionInfo":{"status":"ok","timestamp":1577710063819,"user_tz":-60,"elapsed":5313,"user":{"displayName":"Asbjørn Wulff Helge","photoUrl":"","userId":"00062630824514689988"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# Script for loading and preparing data from .stories \n","\n","# Defining function for loading data\n","def load_data(data_file):\n","    file = open(data_file, encoding='utf-8') \n","    text = file.read()\n","    file.close()\n","    return text\n","        \n","# Function to split the data into data chunks and highligts\n","def split_data(text):\n","    idx = text.find(\"@highlight\")\n","    chunk, highlights = text[:idx], text[idx:].split(\"@highligt\")\n","    highlights = [h.strip() for h in highlights if len(h) > 0]    # Strip white space around highlights\n","    return chunk, highlights\n","\n","# Defining function for iterating through many files in the same directory (direc_name = full directory name)\n","def load_files(direc_name):\n","    data_pairs = list()\n","    for single_file in os.listdir(direc_name):\n","        file_name = direc_name + \"/\" + single_file\n","        text = load_data(file_name)\n","        story, highlights = split_data(text)\n","        data_pairs.append({\"Story\":story,\"Highlights\":highlights})\n","    return data_pairs\n","\n","## Cleaning the data \n","# clean a list of lines\n","def clean_lines(lines,max_length,change_punctuation=False):\n","    max_line = 0\n","    cleaned = list()\n","    # prepare a translation table to remove punctuation\n","    #table = str.maketrans('', '', string.punctuation)\n","    for line in lines:\n","        # If line is empty -> skip to next line\n","        if not line:\n","            continue\n","        # strip source cnn office if it exists\n","        index = line.find('(CNN)')\n","        if index > -1:\n","            line = line[index+len('(CNN) --'):]\n","        # tokenize (really split() on white spaces?) on white space\n","        line = line.split()\n","        if change_punctuation:\n","            line = [\".\" if x==\"@highlight\" else x for x in line]\n","            if line[0]==\".\":\n","                del(line[0])\n","        max_line += len(line)\n","        # convert to lower case\n","        line = [word.lower() for word in line]\n","        # store as string\n","        cleaned.append(' '.join(line))\n","    if change_punctuation:\n","        cleaned[0] = cleaned[0].replace(\" .\",\".\")\n","    if max_line > max_length:\n","        max_length = max_line\n","    cleaned = [\" \".join(cleaned)]\n","    return cleaned, max_length\n","\n","if os.path.isfile('/content/gdrive/My Drive/Deep Learning/VJ_og_AH/Data/cnn_dataset_full.pkl'):  # If there is already a pickle -> load pickle\n","    # load from file\n","    data_pairs = pickle.load(open('/content/gdrive/My Drive/Deep Learning/VJ_og_AH/Data/cnn_dataset_full.pkl', 'rb'))\n","    print('Loaded Data Pairs %d' % len(data_pairs))  # Last time it was 20443\n","    max_length_story = 400\n","    max_length_high = 100\n","else:   # If there is not a pickle -> load data and write pickle\n","    direc = \"/content/gdrive/My Drive/Deep Learning/VJ_og_AH/Data/cnn/stories\"\n","    data_pairs = load_files(direc)\n","    # clean stories\n","    max_length_story = 0\n","    max_length_high = 0\n","    for num,example in enumerate(data_pairs):\n","        example['Story'],max_length_story = clean_lines(example['Story'].split('\\n'),\n","                                                        max_length_story)\n","        example['Highlights'],max_length_high = clean_lines(example['Highlights'],\n","                                                       max_length_high,change_punctuation=True)\n","    print(\"Length of dataset is {}\".format(len(data_pairs)))\n","    print(max_length_story)\n","    print(max_length_high)\n","    # save to file\n","    pickle.dump(data_pairs, open('cnn_dataset.pkl', 'wb'))\n","    \n","print(data_pairs[1][\"Story\"])\n","print(data_pairs[1][\"Highlights\"])\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Loaded Data Pairs 92579\n","['usain bolt rounded off the world championships sunday by claiming his third gold in moscow as he anchored jamaica to victory in the men\\'s 4x100m relay. the fastest man in the world charged clear of united states rival justin gatlin as the jamaican quartet of nesta carter, kemar bailey-cole, nickel ashmeade and bolt won in 37.36 seconds. the u.s finished second in 37.56 seconds with canada taking the bronze after britain were disqualified for a faulty handover. the 26-year-old bolt has now collected eight gold medals at world championships, equaling the record held by american trio carl lewis, michael johnson and allyson felix, not to mention the small matter of six olympic titles. the relay triumph followed individual successes in the 100 and 200 meters in the russian capital. \"i\\'m proud of myself and i\\'ll continue to work to dominate for as long as possible,\" bolt said, having previously expressed his intention to carry on until the 2016 rio olympics. victory was never seriously in doubt once he got the baton safely in hand from ashmeade, while gatlin and the united states third leg runner rakieem salaam had problems. gatlin strayed out of his lane as he struggled to get full control of their baton and was never able to get on terms with bolt. earlier, jamaica\\'s women underlined their dominance in the sprint events by winning the 4x100m relay gold, anchored by shelly-ann fraser-pryce, who like bolt was completing a triple. their quartet recorded a championship record of 41.29 seconds, well clear of france, who crossed the line in second place in 42.73 seconds. defending champions, the united states, were initially back in the bronze medal position after losing time on the second handover between alexandria anderson and english gardner, but promoted to silver when france were subsequently disqualified for an illegal handover. the british quartet, who were initially fourth, were promoted to the bronze which eluded their men\\'s team. fraser-pryce, like bolt aged 26, became the first woman to achieve three golds in the 100-200 and the relay. in other final action on the last day of the championships, france\\'s teddy tamgho became the third man to leap over 18m in the triple jump, exceeding the mark by four centimeters to take gold. germany\\'s christina obergfoll finally took gold at global level in the women\\'s javelin after five previous silvers, while kenya\\'s asbel kiprop easily won a tactical men\\'s 1500m final. kiprop\\'s compatriot eunice jepkoech sum was a surprise winner of the women\\'s 800m. bolt\\'s final dash for golden glory brought the eight-day championship to a rousing finale, but while the hosts topped the medal table from the united states there was criticism of the poor attendances in the luzhniki stadium. there was further concern when their pole vault gold medalist yelena isinbayeva made controversial remarks in support of russia\\'s new laws, which make \"the propagandizing of non-traditional sexual relations among minors\" a criminal offense. she later attempted to clarify her comments, but there were renewed calls by gay rights groups for a boycott of the 2014 winter games in sochi, the next major sports event in russia.']\n","[\"usain bolt wins third gold of world championship. anchors jamaica to 4x100m relay victory. eighth gold at the championships for bolt. jamaica double up in women's 4x100m relay\"]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wqiVB8UfW5Z_","colab_type":"code","colab":{}},"source":["def create_dataloader(data_pairs, load_data, vocab_size):\n","    # Initiating fields for torchtext data handling\n","    en = spacy.load('en_core_web_sm')   # Load spacy's english \n","    def tokenize_en(sentence):\n","        return [tok.text for tok in en.tokenizer(sentence)]\n","\n","    Story_TEXT = torchtext.data.Field(tokenize=tokenize_en,fix_length=max_length_story)\n","\n","    Highlights_TEXT = torchtext.data.Field(tokenize=tokenize_en, fix_length=max_length_high,\n","                                          init_token = \"<sos>\", eos_token = \"<eos>\")\n","    \n","    # If CSV files are present in the folder then skip this\n","    if load_data == False:\n","        df = pd.DataFrame(data_pairs,columns=[\"Story\",\"Highlights\"])\n","\n","        # Create a small training and test set for practice \n","        #small_set,_ = train_test_split(df, test_size=2)\n","        train,val = train_test_split(df, test_size=0.1)\n","\n","        # Preparing CSV for torchtext\n","        train.to_csv(\"train_.csv\", index=False)\n","        val.to_csv(\"val_.csv\", index=False)\n","    \n","        # Associate the text in the 'Story' column with the Story_TEXT field and 'Highlights' with Highlights_TEXT\n","        data_fields = [('Story', Story_TEXT), ('Highlights', Highlights_TEXT)]\n","        train,val = torchtext.data.TabularDataset.splits(path='./', train='train_AH.csv',\n","                                                        validation='val_AH.csv',\n","                                                        format='csv', fields=data_fields)  \n","        \n","        # Building the vocabulary\n","        Highlights_TEXT.build_vocab(train.Story, train.Highlights, min_freq=1) # min_freq defines the minimum occurency of a word for it to be included in the vocab\n","        Story_TEXT.vocab = Highlights_TEXT.vocab\n","        vocab_model = Story_TEXT.vocab.itos[0:vocab_size]\n","\n","    else: \n","        if os.path.isfile('val_AH.csv') == False or os.path.isfile('train_AH.csv') == False:\n","            raise Exception(\"CSV file(s) are not present in the current folder\")\n","        # Associate the text in the 'Story' column with the Story_TEXT field and 'Highlights' with Highlights_TEXT\n","        data_fields = [('Story', Story_TEXT), ('Highlights', Highlights_TEXT)]\n","        train,val = torchtext.data.TabularDataset.splits(path='./', train='train_AH.csv',\n","                                                        validation='val_AH.csv',\n","                                                        format='csv', fields=data_fields) \n","\n","    # Making the iterator for training\n","    batch_size = 16\n","    train_loader = torchtext.data.BucketIterator(train, batch_size=batch_size,\n","                                              shuffle=True,repeat=False)\n","    val_loader = torchtext.data.BucketIterator(val, batch_size=batch_size,\n","                                              shuffle=True,repeat=False)\n","    \n","    not_twice = True\n","    \n","    return train_loader, val_loader, Story_TEXT, Highlights_TEXT, batch_size, not_twice "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3IsK82HlA3Rf","colab":{}},"source":["## Creating a seq2seq network\n","def create_networks(hidden_size, vocab_size, input_length):\n","    class Encoder(nn.Module):\n","        def __init__(self,hidden_size,input_size,bidirectional,maximum_length = input_length):\n","            super(Encoder,self).__init__()\n","            self.input_size = input_size\n","            self.hidden_size = hidden_size\n","            self.input_length = input_length\n","            self.bidirectional = bidirectional\n","\n","            self.embedding = nn.Embedding(input_size,hidden_size)\n","            \n","            self.encoder = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, bidirectional=bidirectional)  \n","            \n","        def forward(self,x,hidden,Story_TEXT):\n","\n","            batch_padding = self.input_length - (x == Story_TEXT.vocab.stoi[\"<pad>\"]).sum(dim=0)\n","\n","            embedded_input = self.embedding(x)\n","\n","            pack_padded_inp  = nn.utils.rnn.pack_padded_sequence(embedded_input, batch_padding, enforce_sorted = False)\n","\n","            packed_output, ht = self.encoder(pack_padded_inp,hidden)\n","\n","            output,_ = nn.utils.rnn.pad_packed_sequence(packed_output,padding_value=Story_TEXT.vocab.stoi[\"<pad>\"],\n","                                                      total_length= input_length)\n","            \n","            outputs = output[:, :, :self.hidden_size] + output[:, : ,self.hidden_size:]\n","            ht = (ht[0, :, :] + ht[1, : ,:]).unsqueeze(0)\n","\n","            return outputs, ht\n","          \n","    # Defining parameters for attention decoder\n","    class AttnDecoderRnn(nn.Module):\n","        def __init__(self, hidden_size, output_size, input_vocab_size, bidirectional, dropout_p=0.1, maximum_length = input_length):\n","            super(AttnDecoderRnn, self).__init__()\n","\n","            self.hidden_size = hidden_size\n","            self.output_size = output_size\n","            #self.dropout_p = dropout_p\n","            self.input_vocab_size = input_vocab_size\n","            self.input_length = maximum_length\n","            self.bidirectional = bidirectional\n","\n","            if bidirectional == True:\n","                factor = 2\n","            else: \n","                factor = 1\n","\n","            #self.dropout_lin = nn.Dropout(dropout_p)\n","\n","            self.embedding = nn.Embedding(input_vocab_size,hidden_size)\n","\n","            self.decoder = nn.GRU(hidden_size, hidden_size, bidirectional=bidirectional)\n","\n","            self.W_out_enc = nn.Linear(hidden_size*factor,hidden_size*factor,bias=False)\n","            self.W_hidden = nn.Linear(hidden_size*factor,hidden_size*factor)\n","            self.W_vector = nn.Linear(hidden_size*factor,1,bias=False)\n","\n","            self.lin_1 = nn.Linear(hidden_size*2*factor,output_size) \n","            \n","        def forward(self, input_var, hidden, encoder_output): # uses decoder's input and hidden state as input.\n","\n","            embedded_input = self.embedding(input_var)\n","\n","            _, hidden_decoder = self.decoder(embedded_input,hidden)\n","\n","            awob = self.W_out_enc(encoder_output)\n","            awb = self.W_hidden(hidden_decoder)\n","            e_i = self.W_vector(torch.tanh(awob + awb))\n","\n","            attn_dist = F.softmax(e_i,dim=0)\n","            context_vector = torch.matmul(attn_dist.permute(1,2,0),encoder_output.permute(1,0,2))\n","\n","            out_lin_1 = self.lin_1(torch.cat((hidden_decoder,context_vector.permute(1,0,2)),2))\n","\n","            return out_lin_1[0], hidden_decoder, attn_dist\n","\n","    def weight_initialization(m):\n","        if type(m)==nn.Linear:\n","            init.xavier_uniform_(m.weight)\n","\n","    net_enc = Encoder(hidden_size,vocab_size,bidirectional=True)\n","    net_dec = AttnDecoderRnn(hidden_size,vocab_size,vocab_size,bidirectional=False)\n","\n","    net_dec.apply(weight_initialization)\n","\n","    if torch.cuda.is_available():\n","        print('##converting networks to cuda-enabled')\n","        net_enc.cuda()\n","        net_dec.cuda()\n","\n","    print(net_enc)\n","    print(net_dec)\n","          \n","    numbers = 0\n","    for param in net_enc.parameters():\n","        numbers += torch.numel(param)\n","    print(\"Number of parameters in encoder: {}\".format(numbers))\n","    for param in net_dec.parameters():\n","        numbers += torch.numel(param)\n","    print(\"Number of parameters in networks: {}\".format(numbers))\n","\n","    return net_enc, net_dec\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xyxZU7o3W5aH","colab_type":"code","colab":{}},"source":["## Defining loss function and optimizer\n","def create_LF_OP(learning_rate, padding_idx,net_enc,net_dec):\n","    class_weights = get_variable(torch.ones(vocab_size))   # Constructed to ignore padding \n","    class_weights[padding_idx] = 0\n","\n","    LEARNING_RATE = learning_rate\n","    criterion = nn.CrossEntropyLoss(weight=class_weights)        \n","\n","    optim_net_enc = optim.Adam(net_enc.parameters(), lr=LEARNING_RATE)\n","    optim_net_dec = optim.Adam(net_dec.parameters(), lr=LEARNING_RATE)\n","\n","    return criterion, optim_net_enc, optim_net_dec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hbESWDbW5aL","colab_type":"code","colab":{}},"source":["def training_loop(ep_check,epochs,max_length_story, max_length_high,batch_size,\n","                  hidden_size,vocab_size,plot_loss,batch_counter,Highlights_TEXT,\n","                  Story_TEXT,vocab_model,net_enc,net_dec,criterion,optim_net_enc,\n","                  optim_net_dec,train_loader):  \n","    net_enc.train()\n","    net_dec.train()   \n","    ## Training the network\n","    for ep in range(ep_check,ep_check+epochs):  \n","        train_loss = 0\n","        train_loader.init_epoch()\n","        for batch_num, train_data in enumerate(train_loader):\n","            # Clearing variables to allow for different lenght inputs\n","            t0 = time.time()\n","            input_encoder = []\n","            input_decoder = []\n","            output_encoder = []\n","            output_decoder = []\n","            barack_search = 0\n","            print_out = list()\n","            print_target = list()\n","            loss = 0      # Resetting loss\n","            optim_net_enc.zero_grad()\n","            optim_net_dec.zero_grad()\n","            \n","            if train_data.Story.size() != torch.Size([max_length_story, batch_size]):\n","                continue\n","        \n","            hidden_encoder = get_variable(torch.zeros(2,batch_size,hidden_size)) # Initializing first hidden state of encoder\n","          \n","            input_encoder = get_variable(train_data.Story.long()) # Getting a training sample\n","            targets = get_variable(train_data.Highlights.permute(1,0).long())  # Getting corresponding target sample\n","\n","            input_encoder[input_encoder>(vocab_size-1)] = 0\n","            targets[targets>(vocab_size-1)] = 0        \n","            \n","            output_encoder , hidden_encoder = net_enc(input_encoder,hidden_encoder,Story_TEXT) \n","            hidden_decoder = hidden_encoder     # Getting the last hidden vector = input for the decoder\n","            \n","            input_decoder = get_variable(train_data.Highlights[0,:].long()) # Initializing start value for decoder corresponding to \"SOS\"\n","\n","            t1 = time.time()\n","            for itera in range(max_length_high-1):     # Teacher forced learning\n","                output_decoder, hidden_decoder, attn_dist_ex = net_dec(torch.unsqueeze(input_decoder,0),\n","                                                                  hidden_decoder, output_encoder)\n","                input_decoder = get_variable(torch.tensor([target[itera+1].item() for target in targets]).long())\n","                            \n","                loss += criterion(get_variable(output_decoder),get_variable(input_decoder))\n","\n","                idx = torch.argmax(output_decoder[0])\n","                print_out.append(Highlights_TEXT.vocab.itos[idx])\n","                print_target.append(Highlights_TEXT.vocab.itos[input_decoder[0]])\n","            t2 = time.time()\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(net_enc.parameters(), 1.0)\n","            torch.nn.utils.clip_grad_norm_(net_dec.parameters(), 1.0)\n","            optim_net_enc.step()\n","            optim_net_dec.step()\n","            \n","            train_loss += loss.item()    \n","            t3 = time.time()\n","            if batch_num % 100 == 0 and batch_num > 1: \n","                print(\"Training batch number {0} in epoch number {1} with average batch training error {2}\".format(batch_num,ep+1,train_loss/100))\n","                plot_loss.append(train_loss/100)\n","                train_loss = 0\n","                print(print_out)\n","                print(print_target)     \n","                print(\"Encoder section: {}. Decoder section: {}. Backpropagation section: {}.\".format(t1-t0,t2-t1,t3-t2))\n","\n","        print(\"Epoch number {} with average batch training error {}\".format(ep+1,train_loss/(batch_num+1)))\n","\n","        torch.save({\n","            'epoch': ep+1,\n","            'model_state_dict_enc': net_enc.state_dict(),\n","            'model_state_dict_dec': net_dec.state_dict(),\n","            'optimizer_state_dict_enc': optim_net_enc.state_dict(),\n","            'optimizer_state_dict_dec': optim_net_dec.state_dict(),\n","            'batch_counter': batch_counter,\n","            'plot_loss': plot_loss,\n","            'vocab_high': Highlights_TEXT.vocab,\n","            'vocab_story': Story_TEXT.vocab,\n","            }, 'checkpoint_Al_10_2.1_EP' + str(ep+1) + '.pth.tar')\n","        \n","        print(\"Epoch number: {}\".format(ep+1))\n","\n","\n","def validation_loop(percent_files,net_enc,net_dec,val_loader,max_length_high ,batch_size, vocab_size,\n","                        max_length_story,hidden_size,Story_TEXT,Highlights_TEXT):\n","    net_enc.eval()\n","    net_dec.eval()\n","    with torch.no_grad():\n","        for batch_num, val_data in enumerate(val_loader):\n","            # Clearing variables to allow for different lenght inputs\n","            input_encoder = []\n","            input_decoder = []\n","            print_output = torch.empty(max_length_high ,batch_size, vocab_size)\n","            output_encoder = []\n","            output_decoder = []\n","            summary = []\n","            reference = []\n","            story = []\n","            loss = 0      # Resetting loss\n","            \n","            if val_data.Story.size() != torch.Size([max_length_story, batch_size]):\n","                continue\n","\n","            hidden_encoder = get_variable(torch.zeros(2,batch_size,hidden_size)) # Initializing first hidden state of encoder\n","          \n","            input_encoder = get_variable(val_data.Story.long()) # Getting a training sample\n","            targets = get_variable(val_data.Highlights.permute(1,0).long())  # Getting corresponding target sample\n","            \n","            input_encoder[input_encoder>(vocab_size-1)] = 0\n","            targets[targets>(vocab_size-1)] = 0\n","\n","            output_encoder, hidden_encoder = net_enc(input_encoder, hidden_encoder, Story_TEXT) \n","            hidden_decoder = hidden_encoder     # Getting the context vector = input for the decoder\n","\n","            input_decoder = get_variable(val_data.Highlights[0,:].long()) # Initializing start value for decoder corresponding to \"SOS\"\n","\n","            for itera in range(max_length_high-1):\n","                output_decoder, hidden_decoder, weights = net_dec(torch.unsqueeze(input_decoder,0),\n","                                                                  hidden_decoder, output_encoder)\n","                \n","                input_decoder = get_variable(torch.tensor([torch.argmax(target).item() for target in output_decoder]).long())\n","\n","                print_output[itera] = output_decoder\n","\n","            if batch_num % 50 == 0: \n","                print(\"Validation batch number {0}\".format(batch_num))\n","            \n","            random_num = np.random.uniform()\n","            if random_num > (1-percent_files):\n","                for sentence in range(batch_size):\n","                    for do in range(max_length_story):\n","                        story.append(Story_TEXT.vocab.itos[val_data.Story[do][sentence]])\n","                    story = list(filter(lambda a: a != \"<pad>\", story))\n","                    story = \" \".join(story)\n","                    sto = open(\"/content/gdrive/My Drive/Deep Learning/VJ_og_AH/Data/stories/story\"  + str(batch_num) + str(sentence+1) + \".txt\",'w+')  # Opening a file in write mode automatically clears previous context\n","                    sto.write(story)\n","                    sto.close\n","                    story = []\n","\n","                for sentence in range(batch_size):       # Preparing data for rouge score\n","                    for do in range(len(targets[0])):\n","                        softies = F.softmax(print_output[do][sentence], dim=0)\n","                        val, ind = torch.max(softies,dim=0)\n","                        summary.append(Story_TEXT.vocab.itos[ind])\n","                        reference.append(Highlights_TEXT.vocab.itos[val_data.Highlights.permute(1,0)[sentence][do]])\n","                    if \"<eos>\" in summary:\n","                        eos_idx = summary.index(\"<eos>\")\n","                        summary = summary[0:eos_idx]\n","                    while \"<unk>\" in summary:\n","                        unk_idx = summary.index(\"<unk>\")\n","                        summary[unk_idx] = \"unk\"\n","                    while \"<unk>\" in reference:\n","                        unk_idx = reference.index(\"<unk>\")\n","                        reference[unk_idx] = \"unk\"\n","                    reference = list(filter(lambda a: a != \"<pad>\", reference))\n","                    reference = list(filter(lambda a: a != \"<sos>\", reference))\n","                    reference = list(filter(lambda a: a != \"<eos>\", reference))\n","                    if sentence == 0:\n","                        print(summary)\n","                        print(reference)\n","                    summary = \" \".join(summary)\n","                    reference = \" \".join(reference)\n","                    sum = open(\"/content/gdrive/My Drive/Deep Learning/VJ_og_AH/Data/summaries/summary.A.\" + str(batch_num) + str(sentence+1) + \".txt\",\"w+\")  # Opening a file in write mode automatically clears previous context\n","                    ref = open(\"/content/gdrive/My Drive/Deep Learning/VJ_og_AH/Data/reference/reference.\" + str(batch_num) + str(sentence+1) + \".txt\",\"w+\")\n","                    sum.write(summary)\n","                    ref.write(reference)\n","                    sum.close()\n","                    ref.close()\n","                    summary = []\n","                    reference = []\n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o0sk5JZBS4HU","colab_type":"code","colab":{}},"source":["def train_error(plot_loss):\n","    # Making a plot over the training error\n","    import matplotlib.pyplot as plt\n","    batch_counter = np.linspace(0,30,len(plot_loss))\n","    plt.figure()\n","    plt.plot(batch_counter, plot_loss, 'r', label='Training loss',)\n","    plt.legend()\n","    plt.xlabel(\"Epoch Number\"),plt.ylabel(\"Training error\")\n","    plt.title(\"Training Error with Adam Optimizer\")\n","    plt.show\n","    plt.savefig('Training_Error')\n","    #print(len(plot_loss)/32)\n","    print(np.mean(plot_loss[1330:]))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8k-XZYVSPaI","colab_type":"code","colab":{}},"source":["def where_the_magic_happens(use_saved,model_file,vocab_size,hidden_size, learning_rate,\n","                      training_epochs, train_session, val_session, percent_files,\n","                      train_loader, val_loader, Story_TEXT, Highlights_TEXT, batch_size, print_plot):    \n","    # Defining parameters for network or loading save\n","    epochs = training_epochs\n","    plot_loss = []\n","    batch_counter = 0\n","    ep = 0\n","\n","    if use_saved == True:\n","        checkpoint = torch.load(model_file)\n","        ep_check = checkpoint['epoch']\n","        plot_loss = checkpoint['plot_loss']\n","        batch_counter = checkpoint[\"batch_counter\"]\n","        Highlights_TEXT.vocab = checkpoint['vocab_high']\n","        Story_TEXT.vocab = checkpoint['vocab_story']\n","        vocab_model = Story_TEXT.vocab.itos[0:vocab_size]\n","\n","        net_enc,net_dec = create_networks(hidden_size, vocab_size, max_length_story)\n","        net_enc.load_state_dict(checkpoint['model_state_dict_enc'])\n","        net_dec.load_state_dict(checkpoint['model_state_dict_dec'])\n","        criterion, optim_net_enc, optim_net_dec = create_LF_OP(learning_rate, Highlights_TEXT.vocab.stoi[\"<pad>\"],net_enc,net_dec)\n","        optim_net_enc.load_state_dict(checkpoint['optimizer_state_dict_enc'])    \n","        optim_net_dec.load_state_dict(checkpoint['optimizer_state_dict_dec'])\n","\n","    # Chosing training or validation\n","    if train_session == True:\n","        training_loop(ep_check,epochs,max_length_story, max_length_high,batch_size,hidden_size,vocab_size,plot_loss,batch_counter,Highlights_TEXT,Story_TEXT,\n","                      vocab_model,net_enc,net_dec,criterion,optim_net_enc,optim_net_dec,train_loader)\n","    elif val_session == True:\n","        validation_loop(percent_files,net_enc,net_dec,val_loader,max_length_high ,batch_size, vocab_size,\n","                        max_length_story,hidden_size,Story_TEXT,Highlights_TEXT)\n","    elif print_plot == True:\n","        train_error(plot_loss)\n","    else:\n","        print(\"Script done\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJ-oaI4_6on0","colab_type":"code","outputId":"cb127c91-43c6-4596-c0af-12c7ad6defae","executionInfo":{"status":"error","timestamp":1577711646697,"user_tz":-60,"elapsed":1565765,"user":{"displayName":"Asbjørn Wulff Helge","photoUrl":"","userId":"00062630824514689988"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["use_saved = True\n","model_file = \"checkpoint_Al_10_2.1_EP30.pth.tar\"\n","vocab_size = 50000  # Not a changeable parameter with loaded data. \n","hidden_size = 256 # Not a changeable parameter with loaded data. \n","learning_rate = 0.001  # Not a changeable parameter with loaded data. \n","training_epochs = 10\n","train_session = True\n","val_session = False\n","print_plot = False\n","percent_files = 0.02\n","if not_twice == False:\n","    print(\"Loading Data\")\n","    train_loader, val_loader, Story_TEXT, Highlights_TEXT, batch_size, not_twice = create_dataloader(data_pairs, use_saved, vocab_size)\n","    print(\"Data Loaded\")\n","where_the_magic_happens(use_saved, model_file, vocab_size, hidden_size, learning_rate,\n","                  training_epochs, train_session, val_session, percent_files,\n","                  train_loader, val_loader, Story_TEXT, Highlights_TEXT, batch_size, print_plot)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Loading Data\n","Data Loaded\n","##converting networks to cuda-enabled\n","Encoder(\n","  (embedding): Embedding(50000, 256)\n","  (encoder): GRU(256, 256, bidirectional=True)\n",")\n","AttnDecoderRnn(\n","  (embedding): Embedding(50000, 256)\n","  (decoder): GRU(256, 256)\n","  (W_out_enc): Linear(in_features=256, out_features=256, bias=False)\n","  (W_hidden): Linear(in_features=256, out_features=256, bias=True)\n","  (W_vector): Linear(in_features=256, out_features=1, bias=False)\n","  (lin_1): Linear(in_features=512, out_features=50000, bias=True)\n",")\n","Number of parameters in encoder: 13589504\n","Number of parameters in networks: 52565840\n","Training batch number 100 in epoch number 31 with average batch training error 119.1787387084961\n","['[', \"'\", 'police', 'occurred', 'when', 'incident', 'chief', 'to', 'complaints', 'about', 'protests', '-', 'air', 'drug', 'sales', '.', 'property', '.', 'police', 'police', 'were', 'been', 'arrested', ',', 'viola', '<unk>', ',', '.', 'for', '.', 'police', 'say', '.', 'police', 'shows', 'young', 'man', 'out', ',', 'from', 'the', ',', '3', 'and', 'with', 'he', 'was', '.', '.', 'back', 'back', '.', 'police', ':', 'police', 'terry', 'mahan', 'would', '\"', 'leave', 'pending', 'outcome', 'of', 'internal', 'affairs', \"'\", \"'\", ']', '<eos>', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', 'brutal', 'brutal', 'brutal', 'brutal', 'brutal', 'brutal', 'brutal', 'brutal', 'brutal', '\"', '\"']\n","['[', \"'\", 'incident', 'began', 'when', 'police', 'responded', 'to', 'complaints', 'of', 'open', '-', 'air', 'drug', 'sales', 'in', 'area', '.', '3', 'people', 'had', 'been', 'arrested', 'when', 'viola', 'young', 'arrived', 'asking', 'questions', ',', 'police', 'say', '.', 'video', 'shows', 'young', 'pull', 'arm', 'away', 'from', 'officer', ',', 'walk', 'off', 'before', 'he', '<unk>', 'her', 'in', 'the', 'back', '.', 'police', ':', 'officer', 'terry', 'mahan', 'on', 'paid', 'leave', 'pending', 'outcome', 'of', 'internal', 'affairs', 'probe', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012935638427734375. Decoder section: 0.13571524620056152. Backpropagation section: 0.29914045333862305.\n","Training batch number 200 in epoch number 31 with average batch training error 118.26104309082031\n","['[', '\"', 'roger', 'federer', 'and', 'play', 'in', 'nadal', 'in', 'women', \"'s\", 'indian', 'event', 'indian', 'wells', '.', 'top', 'want', 'progressed', 'between', 'the', 'in', 'the', 'world', '.', 'saturday', '.', 'top', 'swiss', 'no', 'top', '-', 'former', 'no', 'will', 'play', 'in', 'cincinnati', \"'s\", 'final', \"'s\", 'semifinals', 'in', 'roger', '.', '1', 'caroline', 'azarenka', 'in', 'to', 'the', \"'s\", 'capture', '.', '1', 'ranking', 'sharapova', '\"', ']', '<eos>', 'and', 'women', 'women', 'women', 'women', 'match', 'match', 'match', 'women', 'women', 'women', 'women', 'women', 'women', 'women', 'women', ',', ',', ',', ',', ',', 'women', 'women', 'women', 'women', 'women', 'women', 'women', 'women', 'match', 'match', 'match', 'match', 'match', 'match', 'match', 'match', 'match', 'match']\n","['[', '\"', 'roger', 'federer', 'to', 'play', 'rafael', 'nadal', 'in', 'saturday', \"'s\", 'semifinals', 'at', 'indian', 'wells', '.', 'they', 'both', 'progressed', 'with', 'wins', 'over', 'argentine', 'opponents', 'on', 'friday', '.', 'the', 'world', \"'s\", 'top', 'two', 'female', 'players', 'will', 'meet', 'in', 'sunday', \"'s\", 'women', \"'s\", 'final', '.', 'no', '.', '1', 'victoria', 'azarenka', 'takes', 'on', 'russia', \"'s\", 'no', '.', '2', 'maria', 'sharapova', '\"', ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.01236581802368164. Decoder section: 0.1384439468383789. Backpropagation section: 0.2993149757385254.\n","Training batch number 300 in epoch number 31 with average batch training error 118.60352195739746\n","['[', \"'\", 'marijuana', \"'s\", 'marijuana', \"'s\", 'classmates', ',', 'bag', 'of', 'marijuana', '.', 'nearly', '11', '.', 'allege', '2.5', '.', 'be', 'be', 'a', 'marijuana', 'marijuana', 'children', 'grader', 'was', 'saw', \"n't\", 'legally', 'for', 'to', 'boy', 'pope', ',', 'a', 'pot', 'candy', 'bar', ',', 'marijuana', '.', 'marijuana', 'is', 'a', 'available', 'colorado', ',', 'january', '1', 'scheduled', '10', 'one', 'law', 'is', ',', 'school', 'says', '.', 'the', ':', 'parents', 'to', 'make', 'pot', ',', 'lock', 'or', 'key', 'alcohol', 'alcohol', 'would', 'alcohol', ',', 'firearms', '\"', ']', '<eos>', \"'\", '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"']\n","['[', '\"', 'school', 'says', 'boy', 'sold', 'classmates', 'a', 'bag', 'of', 'marijuana', 'for', '$', '11', ',', 'and', 'one', 'could', \"n't\", 'pay', '.', 'the', 'fourth', '-', 'grader', 'who', 'could', \"n't\", 'pay', 'returned', 'the', 'next', 'day', 'with', 'a', 'pot', 'candy', 'bar', 'to', 'trade', '.', 'marijuana', 'became', 'legal', 'in', 'colorado', 'in', 'january', ',', 'so', 'no', 'state', 'law', 'broken', ',', 'school', 'says', '.', 'letter', 'urges', 'parents', 'to', 'keep', 'pot', 'under', 'lock', 'and', 'key', 'like', 'they', 'would', 'alcohol', ',', 'firearms', '\"', ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012421607971191406. Decoder section: 0.13653349876403809. Backpropagation section: 0.299182653427124.\n","Training batch number 400 in epoch number 31 with average batch training error 120.62841751098632\n","['[', \"'\", 'fewer', 'american', 'academy', 'of', 'pediatrics', 'announced', 'distributing', 'guidelines', 'for', 'the', 'literacy', '.', 'fewer', 'say', 'explain', 'children', 'to', 'read', 'children', 'about', 'infants', 'infants', ',', 'children', 'reading', 'children', 'says', '.', 'the', 'shows', '\"', '-', 'income', 'families', 'reading', 'from', 'than', 'than', 'children', 'in', 'parents', '-', 'income', 'families', \"'\", '\"', 'barack', 'obama', 'touted', 'importance', 'of', 'reading', 'out', 'over', 'out', 'the', 'york', \"'\", ']', '<eos>', 'parents', 'children', 'children', 'children', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"']\n","['[', \"'\", 'the', 'american', 'academy', 'of', 'pediatrics', 'announced', 'new', 'guidelines', 'on', 'early', 'literacy', '.', 'doctors', 'should', 'encourage', 'parents', 'to', 'read', 'out', 'loud', 'to', 'infants', 'and', 'children', ',', 'group', 'says', '.', 'research', ':', 'low', '-', 'income', 'children', 'hear', 'fewer', 'words', 'than', 'kids', 'from', 'higher', '-', 'income', 'families', '.', 'president', 'barack', 'obama', 'touted', 'importance', 'of', 'reading', 'out', 'loud', 'in', 'new', 'video', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012397289276123047. Decoder section: 0.1356673240661621. Backpropagation section: 0.2984898090362549.\n","Training batch number 500 in epoch number 31 with average batch training error 122.27851417541504\n","['[', \"'\", 'thousands', 'release', 'of', '550', 'palestinian', 'prisoners', 'held', 'been', 'freed', 'completed', '\"', '\"', 'israel\\\\', \"'s\", 'military', 'says', '.', 'palestinians', 'of', 'palestinians', 'freed', 'the', 'release', 'of', 'including', 'late', 'friday', 'late', 'late', '.', 'the', 'were', 'out', 'in', 'the', 'bank', 'crossing', 'the', 'of', 'the', 'release', 'of', 'including', 'others', '.', 'the', 'prisoner', 'exchange', 'began', 'the', 'october', ',', 'but', 'the', 'release', 'of', '477', 'palestinians', \"'\", ']', '<eos>', \"'\", 'prisons', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu', 'netanyahu']\n","['[', \"'\", 'the', 'release', 'of', '550', 'palestinian', 'prisoners', 'has', 'been', '\"', 'completed', ',', '\"', 'israel\\\\', \"'s\", 'military', 'says', '.', 'thousands', 'of', 'palestinians', 'welcome', 'the', 'release', ',', 'celebrating', 'late', 'into', 'the', 'night', '.', 'clashes', 'break', 'out', 'at', 'west', 'bank', 'crossing', 'ahead', 'of', 'the', 'release', ',', '20', 'injured', '.', 'the', 'prisoner', 'exchange', 'began', 'in', 'october', ',', 'with', 'the', 'release', 'of', '477', 'palestinians', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012578487396240234. Decoder section: 0.13612937927246094. Backpropagation section: 0.299546480178833.\n","Training batch number 600 in epoch number 31 with average batch training error 122.25964294433594\n","['[', \"'\", 'the', 'kidnap', 'victims', 'were', 'rescued', '.', '.', 'mexico', 'mexico', '.', 'the', 'those', 'they', 'of', 'those', 'victims', 'were', 'among', 'shelters', 'mexico', '.', 'the', 'victims', 'is', 'they', 'were', 'migrants', 'were', 'to', 'cross', 'the', 'the', 'united', 'states', \"'\", ']', '<eos>', \"'\", 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants', 'migrants']\n","['[', \"'\", '81', 'kidnap', 'victims', 'were', 'rescued', 'wednesday', 'in', 'northeast', 'mexico', '.', 'all', 'but', 'one', 'of', 'the', 'victims', 'are', 'from', 'central', 'america', '.', 'the', 'group', 'said', 'they', 'were', 'migrants', 'wanting', 'to', 'cross', 'into', 'the', 'united', 'states', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.01269984245300293. Decoder section: 0.13704657554626465. Backpropagation section: 0.2992091178894043.\n","Training batch number 700 in epoch number 31 with average batch training error 122.62184188842774\n","['[', \"'\", 'new', ':', 'pakistan', 'taliban', 'taliban', 'leader', 'it', 'will', 'focus', 'attacks', 'on', 'punjab', 'province', '.', 'pakistan', 'pakistan', 'taliban', 'says', 'maulana', 'fazlullah', 'as', 'their', 'leader', 'leader', '.', 'the', 'was', 'be', 'linked', 'to', 'the', 'assassination', 'of', '.', 'malala', 'yousafzai', '.', 'pakistan', 'was', 'believed', 'to', 'be', 'ordering', 'the', 'from', 'taliban', \"'\", ']', '<eos>', '\"', 'desert', 'desert', 'desert', 'desert', 'desert', 'desert', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', 'swat', 'swat', '\"', '\"', 'swat', 'swat', 'swat', 'swat', 'swat', 'swat', 'swat', 'swat', 'swat', '\"', '\"', 'swat', 'swat', 'swat', 'swat', 'swat']\n","['[', \"'\", 'new', ':', 'the', 'pakistan', 'taliban', 'says', 'they', 'will', 'focus', 'attacks', 'in', 'punjab', 'province', '.', 'the', 'pakistan', 'taliban', 'choose', 'maulana', 'fazlullah', 'as', 'their', 'new', 'leader', '.', 'he', 'might', 'be', 'linked', 'to', 'the', 'assassination', 'attempt', 'against', 'malala', 'yousafzai', '.', 'he', 'is', 'believed', 'to', 'be', 'ordering', 'attacks', 'from', 'afghanistan', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.013683319091796875. Decoder section: 0.13815546035766602. Backpropagation section: 0.2985405921936035.\n","Training batch number 800 in epoch number 31 with average batch training error 124.07902778625488\n","['[', \"'\", 'new', 'child', 'in', 'everyone', 'than', '200', 'people', 'in', 'including', 'of', 'bruises', 'and', 'cuts', '.', 'buildings', 'people', 'was', 'cloud', 'condition', ',', 'out', ',', 'treatment', ',', 'authorities', 'say', '.', 'the', 'mayor', 'says', 'emergency', 'of', 'emergency', 'in', 'the', ',', 'ruptures', 'and', 'fires', 'were', 'out', 'at', 'food', 'least', '160', 'patients', 'are', 'power', \"'\", ']', '<eos>', \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", '.', '.', '.', '.', '.', '.', \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", 'days', 'days', 'days', 'days', 'days', 'days', 'days', 'days']\n","['[', \"'\", 'one', 'hospital', 'treats', 'more', 'than', '170', 'people', ',', 'most', 'suffering', 'bruises', 'and', 'cuts', '.', 'young', 'child', 'in', 'critical', 'condition', 'flown', 'out', 'for', 'treatment', ',', 'authorities', 'say', '.', 'california', 'governor', 'declares', 'state', 'of', 'emergency', '.', 'water', 'main', 'ruptures', 'and', 'fires', 'break', 'out', ';', 'at', 'least', '15,000', 'people', 'without', 'power', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012943267822265625. Decoder section: 0.1388704776763916. Backpropagation section: 0.30001330375671387.\n","Training batch number 900 in epoch number 31 with average batch training error 124.87689025878906\n","['[', \"'\", 'the', 'sisters', 'and', 'iman', 'and', 'siham', 'hashi', ',', 'are', 'debut', 'for', '.', 'he', 'fusion', 'of', 'hip', '-', 'hop', 'and', 'is', \"'s\", 'and', 'the', 'are', 'are', 'also', 'now', 'in', 'together', '.', '.', ']', '<eos>', '.', 'music', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', 'world', 'world', 'world', 'world', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"']\n","['[', \"'\", 'somali', 'sisters', ',', 'iman', 'and', 'siham', 'hashi', ',', 'make', 'up', '<unk>', '.', 'a', 'fusion', 'of', 'hip', '-', 'hop', ',', 'world', 'pop', 'and', '<unk>', ',', 'they', 'are', 'currently', 'finishing', 'debut', 'album', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012567520141601562. Decoder section: 0.13510465621948242. Backpropagation section: 0.29900455474853516.\n","Training batch number 1000 in epoch number 31 with average batch training error 125.94269729614258\n","['[', \"'\", 'new', 'it\\\\', \"'s\", 'not', 'that', 'it\\\\', 'doors', '\"', 'the', 'source', 'enforcement', 'source', 'says', '.', 'the', 'surreptitious', 'recording', '.', 'anonymous', 'says', 'it', 'recorded', 'the', 'websites', 'for', 'the', 'fbi', 'and', 'scotland', 'york', '.', '.', 'the', 'fbi', 'hacking', 'systems', 'have', 'hacked', ',', 'source', 'source', 'enforcement', 'source', 'says', \"'\", ']', '<eos>', '\"', 'information', '\"', ',', ',', ',', 'monitors', 'monitors', \"'\", \"'\", 'monitors', 'monitors', 'monitors', 'monitors', 'monitors', 'monitors', 'monitors', 'monitors', \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"']\n","['[', \"'\", '\"', 'it\\\\', \"'s\", 'not', 'that', 'sophisticated', ',', '\"', 'a', 'law', 'enforcement', 'source', 'says', 'of', 'the', 'surreptitious', 'recording', '.', 'anonymous', 'says', 'it', 'recorded', 'the', 'call', 'between', 'the', 'fbi', 'and', 'new', 'scotland', 'yard', '.', 'no', 'fbi', 'computer', 'systems', 'were', 'hacked', ',', 'a', 'law', 'enforcement', 'official', 'says', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012738227844238281. Decoder section: 0.135986328125. Backpropagation section: 0.2996175289154053.\n","Training batch number 1100 in epoch number 31 with average batch training error 125.0322745513916\n","['[', \"'\", 'new', 'of', 'stray', 'dogs', 'from', 'used', 'in', 'sochi', '.', 'russia', '.', 'vancouver', 'joining', 'olympics', '.', 'the', 'outrage', '.', 'the', 'skier', 'of', '<unk>', 'medalist', 'gus', '<unk>', 'leaves', 'up', 'hosting', 'of', '.', 'the', 'dogs', 'in', 'sochi', 'arrived', 'in', 'sochi', 'and', 'the', 'be', 'monitored', 'for', 'adoption', '.', \"'\", ']', '<eos>', \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", 'sochi', 'sochi', 'sochi', 'sochi', 'sochi', 'sochi', 'sochi']\n","['[', \"'\", 'reports', 'of', 'stray', 'dogs', 'being', 'killed', 'in', 'sochi', ',', 'russia', ',', 'before', 'the', 'olympics', 'fueled', 'public', 'outrage', '.', 'american', 'skier', 'and', 'silver', 'medalist', 'gus', '<unk>', 'made', 'the', 'cause', 'famous', '.', 'ten', 'dogs', 'from', 'sochi', 'arrived', 'in', 'washington', 'and', 'will', 'be', 'available', 'for', 'adoption', 'soon', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.014796733856201172. Decoder section: 0.13782691955566406. Backpropagation section: 0.29901576042175293.\n","Training batch number 1200 in epoch number 31 with average batch training error 124.91782409667968\n","['[', '\"', 'ruth', 'bader', 'ginsburg', \"'s\", 'new', 'on', 'the', 'activism', ',', '.', 'bench', '.', 'be', 'tensions', '.', 'the', 'is', 'organizers', 'docket', 'not', 'the', 'a', ',', 'political', 'still', 'might', 'affirmative', 'action', '.', 'political', '.', 'justices', 'throws', 'will', 'the', 'rock', 'of', 'the', 'contraception', 'from', 'from', 'political', '-', 'is', 'still', '\"', 'justice', 'distrust', 'opinion', 'will', 'the', 'supreme', 'court', 'grants', 'the', \"'\", \"'\", 'bench', 'class', ']', '<eos>', '\"', 'conservatives', ',', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', 'courts', 'courts']\n","['[', '\"', 'ruth', 'bader', 'ginsburg', \"'s\", 'comments', 'about', \"'\", 'activism', \"'\", 'on', 'bench', 'may', 'raise', 'tensions', '.', 'this', 'term', \"'s\", 'docket', 'not', 'as', 'explosive', ',', 'but', 'still', 'includes', 'affirmative', 'action', 'and', 'prayer', '.', 'roberts', 'court', 'shows', 'little', 'sign', 'of', 'moving', 'dramatically', 'away', 'from', 'where', 'it', 'is', 'now', '.', 'americans', \"'\", 'opinion', 'about', 'the', 'supreme', 'court', 'is', 'split', 'down', 'the', 'middle', '\"', ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.01244974136352539. Decoder section: 0.13613080978393555. Backpropagation section: 0.2992873191833496.\n","Training batch number 1300 in epoch number 31 with average batch training error 126.22366943359376\n","['[', \"'\", 'al', ':', 'u.s', 'fired', 'mortars', 'toward', 'mogadishu', ';', 'port', 'of', 'and', 'residential', 'area', '.', 'mortars', 'hit', 'home', 'for', 'a', 'police', 'security', 'officers', '.', 'the', 'least', '9', 'fires', 'in', '27', ',', 'in', 'the', ';', 'the', 'on', 'torn', 'capital', '.', ']', '<eos>', 'for', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', ',', ',', ',', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'al']\n","['[', '\"', 'sources', ':', 'militants', 'fired', 'mortars', 'toward', 'mogadishu', \"'s\", 'port', ',', 'struck', 'residential', 'area', '.', 'mortars', 'hit', 'home', 'for', 'disabled', 'national', 'army', 'officers', '.', 'at', 'least', '9', 'dead', ',', '27', 'wounded', 'in', 'attack', 'in', 'war', '-', 'torn', 'capital', '\"', ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.01256108283996582. Decoder section: 0.13817596435546875. Backpropagation section: 0.2996187210083008.\n","Training batch number 1400 in epoch number 31 with average batch training error 126.48803428649903\n","['[', \"'\", 'sen', 'it\\\\', 'bulk', 'of', 'the', 'debt', 'is', 'a', 'over', 'over', 'years', 'ago', ',', 'he', 'was', 'going', 'minor', ',', '\"', 'publicist', 'says', '.', 'carter', 'owes', '$', '$', '$', 'than', '$', '2', 'million', 'in', 'his', 'he', 'was', '16', '.', 'he', 'filing', 'was', '$', '$', '2,000', '$', 'month', 'and', 'a', 'he', 'document', 'says', '.', 'he', 'is', '$', '2,000', 'million', 'less', 'wallet', 'and', '$', '<unk>', 'in', 'a', 'checking', 'account', ',', 'filing', 'says', '.', ']', '<eos>', '\"', '\"', 'and', ',', '.', '.', \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"']\n","['[', \"'\", '\"', 'the', 'bulk', 'of', 'the', 'debt', 'is', 'from', 'over', '10', 'years', 'ago', 'when', 'he', 'was', 'a', 'minor', ',', '\"', 'publicist', 'says', '.', 'carter', 'owes', 'the', 'irs', 'more', 'than', '$', '1.3', 'million', 'from', 'when', 'he', 'was', '16', '.', 'the', 'singer', 'makes', 'just', '$', '2,000', 'a', 'month', 'touring', ',', 'court', 'document', 'says', '.', 'he', 'has', '$', '60', 'in', 'his', 'wallet', 'and', '$', '<unk>', 'in', 'a', 'checking', 'account', ',', 'filing', 'says', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.013523101806640625. Decoder section: 0.13785362243652344. Backpropagation section: 0.2989530563354492.\n","Training batch number 1500 in epoch number 31 with average batch training error 126.12922340393067\n","['[', \"'\", 'new', 'in', 'be', '30', 'inches', 'of', 'rain', 'areas', '.', 'including', 'says', '.', 'hurricane', \"'s\", 'capture', 'a', 'in', 'florida', 'spots', ',', 'florida', '.', 'lucie', 'county', '.', 'mississippi', ':', 'center', 'is', 'help', 'in', 'in', 'state', 'new', 'season', ',', 'the', 'fay', 'with', 'cnn', \"'s\", 'hurricane', 'tracker', '\"', ']', '<eos>', 'across', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n","['[', '\"', 'rainfall', 'could', 'reach', '30', 'inches', 'in', 'some', 'areas', ',', 'forecast', 'says', '.', '<unk>', 'help', 'rescue', 'people', 'in', 'flooded', 'homes', 'in', 'st', '.', 'lucie', 'county', '.', 'kennedy', 'space', 'center', 'will', 'remain', 'closed', 'for', 'a', 'second', 'day', '.', 'track', 'fay', 'with', 'cnn', \"'s\", 'hurricane', 'tracker', '\"', ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.013140678405761719. Decoder section: 0.13825535774230957. Backpropagation section: 0.29878997802734375.\n","Training batch number 1600 in epoch number 31 with average batch training error 126.69083625793456\n","['[', \"'\", 'margie', 'and', 'dick', 'baby', 'house', 'divorced', 'from', '37', 'years', 'of', 'marriage', '.', 'divorce', 'in', 'the', 'boomers', 'deny', 'split', 'the', 'rise', 'in', 'divorce', 'policy', 'rates', 'marriage', ',', 'he', ',', 'span', '.', 'be', 'contributing', 'factors', '.', 'divorce', 'in', 'middle', 'boomers', '\"', 'the', 'over', 'as', 'divorce', 'shifts', 'explain', 'reasons', \"'\", ']', '<eos>', \"'\", 'women', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', 'women\\\\', 'women\\\\', 'women\\\\', 'women\\\\', 'women\\\\', 'women\\\\', 'women\\\\', 'women\\\\', 'women\\\\', 'women\\\\', 'women\\\\', 'women\\\\', 'women\\\\', 'women\\\\', 'women\\\\', 'women\\\\']\n","['[', \"'\", 'margie', 'and', 'dick', 'white', 'got', 'divorced', 'after', '37', 'years', 'of', 'marriage', '.', 'divorce', 'among', 'baby', 'boomers', 'is', 'on', 'the', 'rise', '.', 'changing', 'ideas', 'about', 'marriage', ',', 'longer', 'life', 'span', 'could', 'be', 'contributing', 'factors', '.', 'rise', 'in', 'single', 'boomers', 'raises', 'issues', 'such', 'as', 'financial', 'and', 'health', 'concerns', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012567758560180664. Decoder section: 0.13597583770751953. Backpropagation section: 0.29876041412353516.\n","Training batch number 1700 in epoch number 31 with average batch training error 127.73004783630371\n","['[', \"'\", 'france', 'lock', 'sebastien', '<unk>', '<unk>', 'for', 'dangerous', 'dangerous', 'tackle', 'on', 'simon', 'shaw', '.', 'he', '<unk>', 'a', 'hearing', 'on', 'monday', 'after', 'defeating', '.', '\"', '\"', 'in', 'to', 'was', 'be', 'next', 'semifinal', '-', 'fourth', 'place', 'place', 'play', '-', 'off', 'jo', 'on', 'later', 'suspended', 'in', ']', '<eos>', 'rugby', 'women', 'women', 'women', 'women', 'women', 'women', 'eliminated', 'eliminated', 'eliminated', 'eliminated', 'eliminated', 'eliminated', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in']\n","['[', \"'\", 'france', 'lock', 'sebastien', '<unk>', 'cited', 'for', 'a', 'dangerous', 'tackle', 'on', 'simon', 'shaw', '.', '<unk>', 'faces', 'disciplinary', 'hearing', 'on', 'monday', 'after', 'incident', 'against', 'england', '.', 'sale', 'forward', 'will', 'miss', 'the', 'third', 'and', 'fourth', '-', 'place', 'play', '-', 'off', 'is', 'he', 'is', 'suspended', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012528657913208008. Decoder section: 0.13798022270202637. Backpropagation section: 0.29956817626953125.\n","Training batch number 1800 in epoch number 31 with average batch training error 130.11094581604004\n","['[', \"'\", 'world', 'star', 'no', '.', '1', 'novak', 'djokovic', 'wins', 'world', 'world', 'sportsman', 'of', 'the', 'year', '.', 'world', 'fans', 'won', 'three', 'grand', 'slam', 'titles', 'during', 'london', 'row', 'during', 'australian', 'grand', 'open', 'title', 'world', 'star', '34-year', 'runner', 'vivian', 'opens', 'wins', 'named', 'by', 'of', 'the', 'year', 'in', 'world', 'champion', 'title', 'of', 'deny', 'in', 'of', 'the', 'year', \"'\", ']', '<eos>', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"']\n","['[', '\"', 'tennis', 'world', 'no', '.', '1', 'novak', 'djokovic', 'named', '<unk>', 'world', 'sportsman', 'of', 'the', 'year', '.', 'serbian', 'has', 'won', 'three', 'grand', 'slam', 'titles', 'in', 'a', 'row', 'including', 'recent', 'australian', 'open', '.', 'kenya', \"'s\", 'distance', 'runner', 'vivian', '<unk>', 'is', 'named', '<unk>', 'of', 'the', 'year', '.', 'european', 'soccer', 'champions', 'barcelona', 'named', 'team', 'of', 'the', 'year', '\"', ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012115240097045898. Decoder section: 0.1340622901916504. Backpropagation section: 0.2987508773803711.\n","Training batch number 1900 in epoch number 31 with average batch training error 127.94159568786621\n","['[', \"'\", 'rory', 'mcilroy', 'is', ',', '68', 'day', 'round', 'for', 'abu', 'dhabi', '.', 'his', 'of', 'the', '-', 'over', '-', 'par', '.', 'tiger', 'no.1', 'rory', 'to', 'new', 'clubs', 'to', 'first', 'time', 'since', 'tiger', 'bumper', 'deal', 'tribute', '.', 'tiger', 'woods', 'struggles', 'first', 'with', 'first', 'first', 'after', 'carding', '70', 'first', '.', 'tiger', 'rose', 'leads', 'jamie', 'donaldson', 'lead', 'way', 'way', 'world', 'five', '-', 'under', '-', 'first', 'round', \"'\", ']', '<eos>', '\"', 'desert', 'desert', 'women', 'desert', 'women', 'women', 'women', 'women', 'women', 'women', 'women', 'women', 'women', 'women', 'women', 'world', 'world', 'world', 'world', 'desert', 'desert', 'desert', 'desert', 'desert', 'women', 'women', 'women', 'vegas', 'vegas', 'vegas', 'desert', 'desert']\n","['[', \"'\", 'rory', 'mcilroy', 'endured', 'a', 'difficult', 'first', 'day', 'in', 'abu', 'dhabi', 'following', 'round', 'of', 'three', '-', 'over', '-', 'par', '.', 'world', 'no.1', 'used', 'his', 'nike', 'clubs', 'for', 'first', 'time', 'since', 'signing', 'bumper', 'pay', 'deal', '.', 'tiger', 'woods', 'finishes', 'level', 'for', 'the', 'day', 'after', 'carding', 'a', '72', '.', 'justin', 'rose', 'and', 'jamie', 'donaldson', 'lead', 'the', 'way', 'on', 'five', '-', 'under', 'after', 'first', 'round', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012080192565917969. Decoder section: 0.13791823387145996. Backpropagation section: 0.2990732192993164.\n","Training batch number 2000 in epoch number 31 with average batch training error 129.8024732208252\n","['[', \"'\", 'nasa', 'says', 'nasa', 'may', 'a', 'risk', 'high', 'risk', '\"', 'for', 'planned', 'march', '2017', 'launch', 'date', '.', 'this', 'space', 'space', 'program', '.', 'agency', 'accountability', 'may', 'estimates', '$', 'may', 'need', 'additional', 'additional', '$', '400', 'million', '.', 'nasa', 'says', 'it', 'may', 'may', 'failed', 'diversions', 'could', 'lead', 'in', 'faster', '6.7', ',', '$', \"'\", ']', '<eos>', ',', ',', ',', \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", 'space', 'space', 'space', 'space', 'space', 'space', 'space', 'space', 'space', 'space', 'space', 'space', 'space', 'space']\n","['[', \"'\", 'audit', 'says', 'nasa', 'is', 'at', '\"', 'high', 'risk', '\"', 'of', 'missing', 'planned', '2017', 'launch', 'date', 'of', 'deep', '-', 'space', 'rocket', '.', 'government', 'accountability', 'office', 'estimates', 'nasa', 'may', 'need', 'an', 'additional', '$', '400', 'million', '.', 'nasa', 'says', 'schedule', 'delays', 'or', 'fund', 'diversions', 'could', 'result', 'in', 'increased', 'costs', 'to', 'taxpayers', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.011897802352905273. Decoder section: 0.13381481170654297. Backpropagation section: 0.2986440658569336.\n","Training batch number 2100 in epoch number 31 with average batch training error 129.37270965576172\n","['[', \"'\", 'heriberto', 'pagan', ',', '86', ',', 'shot', 'in', 'grandson', 'shot', 'his', 'himself', 'girlfriend', ',', 'girlfriend', '.', 'a', 'say', '.', 'police', 'shooting', 'occurred', 'be', 'been', 'in', 'by', 'his', ',', ',', 'a', 'law', 'enforcement', 'source', 'says', '.', 'pagan', 'fatally', 'shot', 'himself', 'in', 'head', 'head', ';', 'his', 'shooting', 'suicide', 'he', ']', '<eos>', '\"', 'his', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n","['[', '\"', 'heriberto', 'pagan', ',', '86', ',', 'shot', 'his', 'grandson', 'and', 'killed', 'his', 'grandson', \"'s\", 'girlfriend', ',', 'police', 'say', '.', 'the', 'shooting', 'may', 'have', 'been', 'triggered', 'by', 'housing', 'dispute', ',', 'a', 'law', 'enforcement', 'source', 'says', '.', 'pagan', 'fatally', 'shot', 'himself', 'in', 'the', 'head', 'after', 'the', 'double', 'shooting', '\"', ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012017011642456055. Decoder section: 0.1344897747039795. Backpropagation section: 0.29880452156066895.\n","Training batch number 2200 in epoch number 31 with average batch training error 129.13521461486818\n","['[', \"'\", 'valentino', 'rossi', 'wins', 'italian', 'marino', 'italian', 'at', 'rosa', 'circuit', 'marquez', 'victory', 'of', '2014', 'and', 'italian', 'legend', 'valentino', 'rossi', 'credited', 'with', 'points', 'of', 'the', 'championship', 'title', '.', 'championship', 'leader', 'marc', 'marquez', 'finishes', 'on', 'finishes', 'second', 'victory', ']', '<eos>', 'finish', 'desert', 'desert', 'desert', 'desert', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati', 'ducati']\n","['[', \"'\", 'valentino', 'rossi', 'wins', 'san', 'marino', 'motogp', 'at', '<unk>', '.', 'first', 'win', 'of', 'season', 'for', 'italian', 'legend', '.', 'rossi', 'passes', '5,000', 'points', 'in', 'world', 'championship', 'classes', '.', 'championship', 'leader', 'marc', 'marquez', 'crashes', 'and', 'finishes', '15th', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.013152599334716797. Decoder section: 0.13718199729919434. Backpropagation section: 0.29872560501098633.\n","Training batch number 2300 in epoch number 31 with average batch training error 128.9565242767334\n","['[', \"'\", 'the', 'kelly', 'de', '\"', \"haven\\\\'t\", 'been', 'with', 'their', 'in', 'terror', 'to', 'fighting', 'terrorism', 'campaign', 'new', 'york', 'city', '.', 'the', 'de', 'blasio', 'intends', 'to', 'counterterrorism', 'briefing', 'on', 'august', '29', ',', 'officials', 'spokesman', 'says', '.', 'the', 'manager', 'invites', '\"', 'would', 'would', 'to', '\"', 'citizens', 'and', 'bridges', 'and', 'new', 'elsewhere', 'terrorism', '.', 'christine', 'quinn', 'has', 'briefings', '\"', ']', 'and', 'the', 'crime', '\"', '\"', 'writer', 'mayor', 'says', \"'\", ']', '<eos>', 'city', 'terror', 'terror', 'terror', 'terror', 'terror', 'terror', 'terror', 'terror', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism', 'counterterrorism']\n","['[', \"'\", 'ray', 'kelly', ':', 'candidates', \"haven\\\\'t\", 'met', 'with', 'him', 'about', 'plans', 'for', 'fighting', 'terror', 'in', 'new', 'york', 'city', '.', 'bill', 'de', 'blasio', 'sought', 'a', 'counterterrorism', 'briefing', 'on', 'august', '29', ',', 'a', 'spokesman', 'says', '.', 'campaign', 'site', ':', 'bill', 'thompson', 'aims', 'to', 'protect', 'roads', ',', 'bridges', 'and', 'tunnels', 'from', 'terrorism', '.', 'christine', 'quinn', 'has', 'briefings', '\"', 'regularly', 'with', 'the', 'nypd', ',', '\"', 'her', 'spokesman', 'says', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012520074844360352. Decoder section: 0.13577818870544434. Backpropagation section: 0.29898500442504883.\n","Training batch number 2400 in epoch number 31 with average batch training error 132.52367752075196\n","['[', \"'\", 'tom', '.', 'tom', 'coburn', 'says', 'on', 'bill', 'to', 'cover', 'bill', ',', '.', '<unk>', 'law', 'law', '.', 'the', 'would', 'require', 'a', ',', 'the', 'law', 'insurance', 'who', 'has', 'expected', 'six', 'from', 'with', 'commissioner', 'leader', 'says', '.', 'taxpayer', 'bill', 'will', 'expire', 'at', 'the', \"'s\", 'end', 'of', 'speculation', 'are', \"they\\\\'ll\", 'have', 'insurance', 'law', 'cover', 'bill', \"'\", 'year', '\"', 'nfl', 'spokesman', 'says', 'speculation', 'centers', 'the', 'super', 'bowl', 'will', 'be', 'played', '\"', '\"', 'not', 'true', '\"', \"'\", ']', '<eos>', \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\"]\n","['[', \"'\", 'sen', '.', 'tom', 'coburn', 'insisted', 'on', 'changes', 'to', 'a', 'bill', '<unk>', 'a', 'terrorism', 'insurance', 'program', '.', 'changes', 'would', 'require', 'action', 'by', 'the', 'house', ',', 'which', 'is', 'on', 'break', ',', 'senate', 'majority', 'leader', 'says', '.', 'the', 'bill', 'will', 'expire', 'at', 'year\\\\', \"'s\", 'end', ';', 'republicans', 'say', \"they\\\\'ll\", 'make', 'the', 'insurance', 'a', 'priority', 'next', 'month', '.', 'nfl', 'spokesman', ':', 'speculation', 'that', 'the', 'super', 'bowl', 'will', 'be', 'canceled', 'is', '\"', 'not', 'true', '\"', \"'\", ']', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","Encoder section: 0.012285709381103516. Decoder section: 0.13565921783447266. Backpropagation section: 0.2975895404815674.\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-cae4fe2481d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m where_the_magic_happens(use_saved, model_file, vocab_size, hidden_size, learning_rate,\n\u001b[1;32m     16\u001b[0m                   \u001b[0mtraining_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                   train_loader, val_loader, Story_TEXT, Highlights_TEXT, batch_size, print_plot)\n\u001b[0m","\u001b[0;32m<ipython-input-9-82a7434ab32a>\u001b[0m in \u001b[0;36mwhere_the_magic_happens\u001b[0;34m(use_saved, model_file, vocab_size, hidden_size, learning_rate, training_epochs, train_session, val_session, percent_files, train_loader, val_loader, Story_TEXT, Highlights_TEXT, batch_size, print_plot)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_session\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         training_loop(ep_check,epochs,max_length_story, max_length_high,batch_size,hidden_size,vocab_size,plot_loss,batch_counter,Highlights_TEXT,Story_TEXT,\n\u001b[0;32m---> 29\u001b[0;31m                       vocab_model,net_enc,net_dec,criterion,optim_net_enc,optim_net_dec,train_loader)\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mval_session\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         validation_loop(percent_files,net_enc,net_dec,val_loader,max_length_high ,batch_size, vocab_size,\n","\u001b[0;32m<ipython-input-7-cc813088faab>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(ep_check, epochs, max_length_story, max_length_high, batch_size, hidden_size, vocab_size, plot_loss, batch_counter, Highlights_TEXT, Story_TEXT, vocab_model, net_enc, net_dec, criterion, optim_net_enc, optim_net_dec, train_loader)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 output_decoder, hidden_decoder, attn_dist_ex = net_dec(torch.unsqueeze(input_decoder,0),\n\u001b[1;32m     44\u001b[0m                                                                   hidden_decoder, output_encoder)\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0minput_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitera\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-cc813088faab>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 output_decoder, hidden_decoder, attn_dist_ex = net_dec(torch.unsqueeze(input_decoder,0),\n\u001b[1;32m     44\u001b[0m                                                                   hidden_decoder, output_encoder)\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0minput_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitera\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"SFdqQMCLu4nn","colab_type":"code","outputId":"1d44f6ca-5230-4a77-e3e9-3348edccd51d","executionInfo":{"status":"ok","timestamp":1575452254914,"user_tz":-60,"elapsed":1561,"user":{"displayName":"Asbjørn Wulff Helge","photoUrl":"","userId":"00062630824514689988"}},"colab":{"base_uri":"https://localhost:8080/","height":467}},"source":["# Histogram of attention. First part should be moved into the trainingloop(). Second part is for constructing the histogram\n","if barack_search == 0:\n","    for bat in range(batch_size):\n","        if Highlights_TEXT.vocab.itos[targets[bat][itera].item()] == \"barack\":\n","            attn_barack = attn_dist_ex.permute(1,2,0).squeeze()\n","            barack_search = attn_barack[bat]\n","            barack_story = input_encoder[:,bat]\n","            print(\"yyyyyeeeehaaaaa\")\n","\n","print(Highlights_TEXT.vocab.stoi[\"obama\"])\n","print(Highlights_TEXT.vocab.itos[3592])\n","print(barack_story[62:67])\n","values, indices = torch.topk(barack_search,5)\n","print(indices)\n","for i in range(5):\n","    print(Highlights_TEXT.vocab.itos[barack_story[indices[i]]])\n","\n","barack = [barack_story,barack_search]\n","pickle.dump(barack, open('barack_search.pkl', 'wb'))\n","\n","\n","bardas = list(range(indices[0]-10,indices[0]+10))\n","print(bardas)\n","words = []\n","valius = []\n","for i in range(len(bardas)):\n","    words.append(Highlights_TEXT.vocab.itos[barack_story[bardas[i]]])\n","    valius.append(barack_search[bardas[i]])\n","print(valius)\n","\n","plt.bar(bardas, valius, align='center', alpha=0.5)\n","plt.xticks(bardas, words, rotation=45)\n","plt.ylabel('Attention Score')\n","plt.title('Attention - Previous word: {}'.format(\"barack\"))\n","plt.gcf().subplots_adjust(bottom=0.35)\n","plt.savefig('attn_barack.png', dpi=400)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["137\n","israel\\\n","tensor([   14,   277,     9,  4800, 13044], device='cuda:0')\n","tensor([58, 57, 49,  5, 35])\n","obama\n","barack\n","presidential\n","john\n","berlin\n","[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]\n","[tensor(0.0321, grad_fn=<SelectBackward>), tensor(0.0503, grad_fn=<SelectBackward>), tensor(0.0116, grad_fn=<SelectBackward>), tensor(9.8516e-06, grad_fn=<SelectBackward>), tensor(0.0001, grad_fn=<SelectBackward>), tensor(0.0016, grad_fn=<SelectBackward>), tensor(0.0006, grad_fn=<SelectBackward>), tensor(0.0143, grad_fn=<SelectBackward>), tensor(0.0011, grad_fn=<SelectBackward>), tensor(0.0829, grad_fn=<SelectBackward>), tensor(0.0924, grad_fn=<SelectBackward>), tensor(0.0011, grad_fn=<SelectBackward>), tensor(0.0005, grad_fn=<SelectBackward>), tensor(0.0004, grad_fn=<SelectBackward>), tensor(0.0004, grad_fn=<SelectBackward>), tensor(0.0026, grad_fn=<SelectBackward>), tensor(4.2326e-05, grad_fn=<SelectBackward>), tensor(0.0019, grad_fn=<SelectBackward>), tensor(0.0015, grad_fn=<SelectBackward>), tensor(9.0885e-05, grad_fn=<SelectBackward>)]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAADvCAYAAADoxUNwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTVJBJEmSJDX5fy5tN4D4J1PgAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"GZSi4MaWoQ1u","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}